{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketmaurya/pytorch-gans/blob/main/DCGAN/dcgan_lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViausjeIZzgf"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketmaurya/pytorch-gans/blob/main/DCGAN/dcgan_lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmDkIg6KZzgk"
      },
      "source": [
        "# DC-GAN\n",
        "\n",
        "**Paper: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**\n",
        "\n",
        "**Link: https://arxiv.org/abs/1511.06434**\n",
        "\n",
        "**Authors: Alec Radford, Luke Metz, Soumith Chintala**\n",
        "\n",
        "## Abstract\n",
        "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU1amexlZzgl"
      },
      "source": [
        "# What is DCGAN?\n",
        "DCGAN (Deep Convolutional Generative Adversarial Network) is created by Alec Radford, Luke Metz and Soumith Chintala in 2016 to train Deep Generative Adversarial Networks. In the [DCGAN paper](https://arxiv.org/abs/1511.06434), the authors trained the network to produce fake faces of celebrities and fake bedroom images.\n",
        "\n",
        "The architecture consists of two networks - Generator and Discriminator. Generator is the heart of GANs. It produces real looking fake images from random noise. \n",
        "\n",
        "Discriminator wants the real and fake image distributions to be as far as possible while the Generator wants to reduce the distance between the real and fake image distribution.\n",
        "In simple words, the Generator tries to fool the Discriminator by producing real looking images while the Discriminator tries to catch the fake images from the real ones.\n",
        "\n",
        "\n",
        "\n",
        "| ![Picture from paper](https://raw.githubusercontent.com/aniketmaurya/ml-resources/master/images/dcgan-vector-arithmetic.png) |\n",
        "| :-: |\n",
        "| *Vector arithmetic for visual concepts. Source: Paper* |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "wfLCwbewZzgl"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "BeDqO1TcZzgm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cryptography-41.0.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cffi-1.16.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cryptography-41.0.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cffi-1.16.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "noDipj_oZzgm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pytorch_lightning as pl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udh1FMo2Zzgm"
      },
      "source": [
        "### [Optional] \n",
        "CelebA dataset is available on Kaggle and torchvision datasets but the latter sometimes doesn't work.\n",
        "Run the code below if you want to download dataset from Kaggle.\n",
        "\n",
        "\n",
        "First you will have to load your kaggle key at /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "ecse-LkdZzgn",
        "outputId": "a8ea4cb0-8b04-4dab-b6f3-553a05b90f7d"
      },
      "outputs": [],
      "source": [
        "# run this if want to train on kaggle dataset\n",
        "# copy kaggle key to /root/.kaggle/kaggle.json\n",
        "\n",
        "# import os\n",
        "# os.makedirs('/root/.kaggle')\n",
        "# !cp ./kaggle.json /root/.kaggle/kaggle.json\n",
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "# !unzip -qq ./celeba-dataset.zip\n",
        "path = \"/Users/kody/Documents/data/celeba/img_align_celeba/img_align_celeba\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "jmQ1euUJZzgo"
      },
      "outputs": [],
      "source": [
        "def show_tensor_images(image_tensor,\n",
        "                       num_images=25,\n",
        "                       size=(3, 64, 64),\n",
        "                       ret=False):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    if ret:\n",
        "        return image_grid.permute(1, 2, 0).squeeze()\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blGgWfe6Zzgo"
      },
      "source": [
        "# Training details from the paper\n",
        "**Preprocessing**: Images are scaled to be in range of tanh activation, [-1, 1].\n",
        "Training was done with a mini-batch size of 128 and Adam optimizer with a learning rate of 0.0002.\n",
        "All the weights initialised with Normal distribution $\\mu(0, 0.02)$.\n",
        "\n",
        "**Authors guidelines:**\n",
        "- All the pooling layers are replaced with strided convolutions.\n",
        "- No fully-connected or pooling layers are used.\n",
        "- Batchnorm used in both Generator and Discriminator\n",
        "- ReLu activation is used for generator for all the layers except the last layer which uses tanh\n",
        "- Discriminator uses LeakyReLu for all the layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "KqCp6okxZzgo"
      },
      "outputs": [],
      "source": [
        "# Configurations are from DCGAN paper\n",
        "z_dim = 100\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "\n",
        "display_step = 500\n",
        "device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKiXTuTEZzgp"
      },
      "source": [
        "### Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "BjRw0ob7Zzgp"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "Dl7OerU9Zzgp"
      },
      "outputs": [],
      "source": [
        "# You can tranform the image values to be between -1 and 1\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "ICXXTtqZZzgp"
      },
      "outputs": [],
      "source": [
        "# Run this if want to load kaggle dataset\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.transform = transform\n",
        "        self.files = glob(f'{path}/*')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file = self.files[idx]\n",
        "        label = file.split('/')[-1]\n",
        "        image = Image.open(file)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "ds = ImageDataset(path, transform=transform)\n",
        "dataloader = dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "G0nr1ihuZzgq"
      },
      "outputs": [],
      "source": [
        "# Run this if you want to use Pytorch datasets\n",
        "\n",
        "# ds = datasets.CelebA('.',\n",
        "#                     download=True,\n",
        "#                     transform=transform)\n",
        "# dataloader = dl = DataLoader(ds,\n",
        "#                              batch_size=batch_size,\n",
        "#                              shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYGqPn_OZzgq"
      },
      "source": [
        "# Network\n",
        "\n",
        "GANs are made up of two entities - Generator and Discriminator.\n",
        "\n",
        "## Generator\n",
        "Generator learns the data distribution and creates fake images.\n",
        "\n",
        "> A Generator consists Transposed Convolution, Batch Normalisation and activation function layer.\n",
        "\n",
        "- First the random noise of size 100 will be reshaped to 100x1x1 (channel first in PyTorch).\n",
        "- It is passed through a Transposed CNN layer which upsamples the input Tensor.\n",
        "- Batch Normalisation is applied.\n",
        "- If the layer is not the last layer then ReLu activation is applied else Tanh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "kNbhcep7cInY"
      },
      "outputs": [],
      "source": [
        "def get_noise(cur_batch_size, z_dim):\n",
        "    noise = torch.randn(cur_batch_size, z_dim, 1, 1, device=device)\n",
        "    return noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "mWnqCV9yZzgq"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, z_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.gen = nn.Sequential(\n",
        "            self.create_upblock(z_dim,\n",
        "                                1024,\n",
        "                                kernel_size=4,\n",
        "                                stride=1,\n",
        "                                padding=0),\n",
        "            self.create_upblock(1024, 512, kernel_size=4, stride=2, padding=1),\n",
        "            self.create_upblock(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            self.create_upblock(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            self.create_upblock(128,\n",
        "                                3,\n",
        "                                kernel_size=4,\n",
        "                                stride=2,\n",
        "                                padding=1,\n",
        "                                final_layer=True),\n",
        "        )\n",
        "\n",
        "    def create_upblock(self,\n",
        "                       in_channels,\n",
        "                       out_channels,\n",
        "                       kernel_size=5,\n",
        "                       stride=2,\n",
        "                       padding=1,\n",
        "                       final_layer=False):\n",
        "        if final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels,\n",
        "                                   out_channels,\n",
        "                                   kernel_size,\n",
        "                                   stride,\n",
        "                                   padding,\n",
        "                                   bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.Tanh()\n",
        "                )\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels,\n",
        "                               out_channels,\n",
        "                               kernel_size,\n",
        "                               stride,\n",
        "                               padding,\n",
        "                               bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True))\n",
        "\n",
        "    def forward(self, noise):\n",
        "        \"\"\"\n",
        "        noise: random vector of shape=(N, 100, 1, 1)\n",
        "        \"\"\"\n",
        "        assert len(noise.shape) == 4, 'random vector of shape=(N, 100, 1, 1)'\n",
        "\n",
        "        return self.gen(noise)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLdNiKtsZzgr"
      },
      "source": [
        "# Discriminator\n",
        "\n",
        "The architecture of a Discriminator is same as that of a normal image classification model.\n",
        "\n",
        "It contains Convolution layers, Activation layer and BatchNormalisation.\n",
        "\n",
        "In the DCGAN paper, strides are used instead of pooling to reduce the size of a kernel. Also there is no Fully Connected layer in the network. Leaky ReLU with leak slope 0.2 is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "VZtcnabJZzgr"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, im_chan=3, hidden_dim=32):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            self.make_disc_block(im_chan, hidden_dim),\n",
        "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_disc_block(hidden_dim * 2, hidden_dim * 4, stride=1),\n",
        "            self.make_disc_block(hidden_dim * 4, hidden_dim * 4, stride=2),\n",
        "            self.make_disc_block(hidden_dim * 4, 1, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_disc_block(self,\n",
        "                        input_channels,\n",
        "                        output_channels,\n",
        "                        kernel_size=4,\n",
        "                        stride=2,\n",
        "                        final_layer=False):\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size,\n",
        "                          stride), nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2))\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size,\n",
        "                          stride))\n",
        "\n",
        "    def forward(self, image):\n",
        "        disc_pred = self.disc(image)\n",
        "        return disc_pred.view(len(disc_pred), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "7OX-fnd2Zzgs"
      },
      "outputs": [],
      "source": [
        "# disc = Discriminator()\n",
        "# disc(torch.randn(10, 3, 64, 64)).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "JO6O608WZzgs"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTr8SKuZzgs"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "Binary Crossentropy loss is used for training DCGAN.\n",
        "\n",
        "### Discriminator Loss\n",
        "As the discriminator wants to increase the distance between Generated and Real distribution, we will train it to give high loss when the generated images is classified as real or when real images are classified as fake.\n",
        "\n",
        "### Generator Loss\n",
        "The BCE loss for Generator will be high when it fails to fool the Discriminator. It will give high loss when the generated image is classified as fake by the discriminator. Note that the Generator never know about real images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2odHPFy-Zzgs"
      },
      "source": [
        "# WHAT IS PYTORCH LIGHTNING?\n",
        "> **Lightning makes coding complex networks simple.\n",
        "Spend more time on research, less on engineering. It is fully flexible to fit any use case and built on pure PyTorch so there is no need to learn a new language.**\n",
        "\n",
        "*A quick refactor will allow you to:*\n",
        "- Run your code on any hardware\n",
        "- Performance & bottleneck profiler\n",
        "- Model checkpointing\n",
        "- 16-bit precision\n",
        "- Run distributed training\n",
        "- Logging\n",
        "- Metrics\n",
        "- Visualization\n",
        "- Early stopping\n",
        "- ... and many more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "_VIG4J5HZzgt"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "WgGjPf-GZzgt"
      },
      "outputs": [],
      "source": [
        "class GAN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self, learning_rate, in_channels=3, hidden_dim=32, z_dim=100, **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.gen = Generator(in_channels, z_dim=z_dim)\n",
        "        self.disc = Discriminator(im_chan=in_channels, hidden_dim=hidden_dim)\n",
        "\n",
        "        self.gen.apply(weights_init)\n",
        "        self.disc.apply(weights_init)\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    def forward(self, noise):\n",
        "        # in lightning, forward defines the prediction/inference actions\n",
        "        return self.gen(noise)\n",
        "\n",
        "    def disc_step(self, x, noise):\n",
        "        \"\"\"\n",
        "        x: real image\n",
        "        \"\"\"\n",
        "        fake_images = self.gen(noise)\n",
        "        # get discriminator outputs\n",
        "        real_logits = self.disc(x)\n",
        "        fake_logits = self.disc(fake_images.detach())\n",
        "        assert (\n",
        "            real_logits.shape == fake_logits.shape\n",
        "        ), f\"Real and fake logit shape are different: {real_logits.shape} and {fake_logits.shape}\"\n",
        "\n",
        "        # real loss\n",
        "        real_loss = criterion(real_logits, torch.ones_like(real_logits))\n",
        "        # fake loss\n",
        "        fake_loss = criterion(fake_logits, torch.zeros_like(fake_logits))\n",
        "        disc_loss = (fake_loss + real_loss) / 2\n",
        "\n",
        "        assert disc_loss is not None\n",
        "        self.log(\"disc_loss\", disc_loss, on_epoch=True, prog_bar=True)\n",
        "        return disc_loss\n",
        "\n",
        "    def gen_step(self, x, noise):\n",
        "        # generate fake images\n",
        "        fake_images = self.gen(noise)\n",
        "\n",
        "        fake_logits = self.disc(fake_images)\n",
        "        fake_loss = criterion(fake_logits, torch.ones_like(fake_logits))\n",
        "\n",
        "        gen_loss = fake_loss\n",
        "\n",
        "        assert gen_loss is not None\n",
        "        self.log(\"gen_loss\", gen_loss, on_epoch=True, prog_bar=True)\n",
        "        return gen_loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        x = real = x\n",
        "        noise = get_noise(real.shape[0], self.z_dim)\n",
        "        assert real.shape[1:] == (\n",
        "            3,\n",
        "            64,\n",
        "            64,\n",
        "        ), f\"batch image data shape is incorrect: {real.shape}\"\n",
        "\n",
        "        # Access your optimizers with self.optimizers()\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "\n",
        "        # Generator optimization\n",
        "        opt_g.zero_grad()\n",
        "        loss_gen = self.gen_step(real, noise)\n",
        "        self.manual_backward(loss_gen)\n",
        "        opt_g.step()\n",
        "\n",
        "        # Discriminator optimization\n",
        "        opt_d.zero_grad()\n",
        "        loss_disc = self.disc_step(real, noise)\n",
        "        self.manual_backward(loss_disc)\n",
        "        opt_d.step()\n",
        "\n",
        "        # Logging\n",
        "        self.log(\"gen_loss\", loss_gen, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"disc_loss\", loss_disc, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        if batch_idx % display_step == 0:\n",
        "            fake_images = self.forward(noise)\n",
        "            show_tensor_images(fake_images)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.learning_rate\n",
        "\n",
        "        opt_g = torch.optim.Adam(self.gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "        opt_d = torch.optim.Adam(self.disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "        return [opt_g, opt_d]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "0dPCHzJ2Zzgt",
        "outputId": "9f4877f8-fe41-49a2-e074-7075cfb3c073"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "model = GAN(learning_rate=lr, z_dim=z_dim)\n",
        "trainer = pl.Trainer(max_epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7f62c42f3c7448eb945e058f3fd70b5d"
          ]
        },
        "id": "VceCzKmyZzgt",
        "outputId": "6475fd79-5074-4ec2-edb1-b95557111e15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name | Type          | Params\n",
            "---------------------------------------\n",
            "0 | gen  | Generator     | 12.7 M\n",
            "1 | disc | Discriminator | 430 K \n",
            "---------------------------------------\n",
            "13.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "13.1 M    Total params\n",
            "52.356    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dde663ad98ed4578b9c2118dcdb08f61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NotImplementedError",
          "evalue": "The operator 'aten::slow_conv_transpose2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dl)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    546\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    547\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1036\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1036\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:242\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    240\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautomatic_optimization\u001b[39m.\u001b[39mrun(trainer\u001b[39m.\u001b[39moptimizers[\u001b[39m0\u001b[39m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmanual_optimization\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    246\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/manual.py:92\u001b[0m, in \u001b[0;36m_ManualOptimization.run\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_start()\n\u001b[1;32m     91\u001b[0m \u001b[39mwith\u001b[39;00m suppress(\u001b[39mStopIteration\u001b[39;00m):  \u001b[39m# no loop to break at this level\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(kwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_end()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/manual.py:112\u001b[0m, in \u001b[0;36m_ManualOptimization.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    111\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    113\u001b[0m \u001b[39mdel\u001b[39;00m kwargs  \u001b[39m# release the batch from memory\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[1;32m/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb Cell 30\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Generator optimization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m opt_g\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m loss_gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_step(real, noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_backward(loss_gen)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m opt_g\u001b[39m.\u001b[39mstep()\n",
            "\u001b[1;32m/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen_step\u001b[39m(\u001b[39mself\u001b[39m, x, noise):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# generate fake images\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     fake_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen(noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     fake_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisc(fake_images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     fake_loss \u001b[39m=\u001b[39m criterion(fake_logits, torch\u001b[39m.\u001b[39mones_like(fake_logits))\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb Cell 30\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mnoise: random vector of shape=(N, 100, 1, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(noise\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrandom vector of shape=(N, 100, 1, 1)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kody/Documents/github/python/pytorch-gans/DCGAN/dcgan_lightning.ipynb#Y145sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen(noise)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    954\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::slow_conv_transpose2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
          ]
        }
      ],
      "source": [
        "trainer.fit(model, dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KobeZDNUZzgu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "dcgan_lightning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
