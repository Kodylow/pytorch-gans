{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKuTFPaw1w9z"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aniketmaurya/pytorch-gans/blob/main/pix2pix/Pix2Pix_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hs5r4ii51w94"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "054Huvnpsd40",
    "outputId": "23409818-f34a-4536-b867-b83f735e8a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxLXDt5M1w95",
    "outputId": "c5e2a049-ef58-447f-8e49-700ab2705c3e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zlPOusgo1w96"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import center_crop\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "el4m3uAw1w97"
   },
   "outputs": [],
   "source": [
    "# New parameters\n",
    "adv_criterion = nn.BCEWithLogitsLoss() \n",
    "recon_criterion = nn.L1Loss() \n",
    "lambda_recon = 200\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "display_step = 200\n",
    "batch_size = 4\n",
    "lr = 0.0002\n",
    "target_size = 256\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yvVVMjYz1w97"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2jnEsFUy1w97"
   },
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor,\n",
    "                       num_images=25,\n",
    "                       size=(3, 64, 64),\n",
    "                       ret=False):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    if ret:\n",
    "        return image_grid.permute(1, 2, 0).squeeze()\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bMJWEkQM1w98"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JM_aHTFM1w98"
   },
   "outputs": [],
   "source": [
    "# !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "# !tar -xvf facades.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKKx1xzf1w98",
    "outputId": "b6f20300-a749-4e5f-91f8-512b4142ce68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "path = './facades/train/'\n",
    "\n",
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, path, target_size=None):\n",
    "        self.filenames = glob(str(Path(path)/'*'))\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        image = Image.open(filename)\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image_width = image.shape[2]        \n",
    "        \n",
    "        real = image[:, :, :image_width // 2]\n",
    "        condition = image[:, :, image_width // 2:]\n",
    "        \n",
    "        target_size = self.target_size\n",
    "        if target_size:\n",
    "            condition = nn.functional.interpolate(condition, size=target_size)\n",
    "            real = nn.functional.interpolate(real, size=target_size)\n",
    "        \n",
    "        return real, condition\n",
    "    \n",
    "    \n",
    "dataset = FacadesDataset(path, target_size=target_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "transforms.functional.to_pil_image(dataset[0][0])\n",
    "transforms.functional.to_pil_image(dataset[0][0]).size\n",
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "b2-IhD2S1w99"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_dropout=False, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "        if use_bn:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            input_channels, input_channels // 2, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            input_channels // 2, input_channels // 2, kernel_size=2, padding=1\n",
    "        )\n",
    "        if use_bn:\n",
    "            self.batchnorm = nn.BatchNorm2d(input_channels // 2)\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = nn.ReLU()\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self, x, skip_con_x):\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv1(x)\n",
    "        skip_con_x = center_crop(skip_con_x, x.shape[-2:])\n",
    "        x = torch.cat([x, skip_con_x], axis=1)\n",
    "        x = self.conv2(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, use_dropout=False, use_bn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels, in_channels * 2, use_dropout, use_bn)\n",
    "        self.conv_block2 = ConvBlock(\n",
    "            in_channels * 2, in_channels * 2, use_dropout, use_bn\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(in_channels, out_channels, kernel-4, strides=2, padding=1, activation=False, batchnorm=False, dropout=False)\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
    "        \n",
    "        if activation:\n",
    "            self.act = nn.ReLU(True)\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32, depth=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=1)\n",
    "\n",
    "        self.conv_final = nn.Conv2d(hidden_channels, out_channels, kernel_size=1)\n",
    "        self.depth = depth\n",
    "\n",
    "        self.contracting_layers = []\n",
    "        self.expanding_layers = []\n",
    "        self.final_act = nn.Tanh()\n",
    "\n",
    "        # encoding/contracting path of the Generator\n",
    "        for i in range(depth):\n",
    "            down_sample_conv = DownSampleConv(\n",
    "                hidden_channels * 2 ** i,\n",
    "            )\n",
    "            self.contracting_layers.append(down_sample_conv)\n",
    "\n",
    "        # decoder/Expanding path of the Generator\n",
    "        for i in range(depth):\n",
    "            upsample_conv = UpSampleConv(\n",
    "                hidden_channels * 2 ** (i + 1), use_dropout=(True if i < 3 else False)\n",
    "            )\n",
    "            self.expanding_layers.append(upsample_conv)\n",
    "\n",
    "        self.contracting_layers = nn.ModuleList(self.contracting_layers)\n",
    "        self.expanding_layers = nn.ModuleList(self.expanding_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        depth = self.depth\n",
    "        contractive_x = []\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        contractive_x.append(x)\n",
    "\n",
    "        for i in range(depth):\n",
    "            x = self.contracting_layers[i](x)\n",
    "            contractive_x.append(x)\n",
    "\n",
    "        for i in range(depth - 1, -1, -1):\n",
    "            x = self.expanding_layers[i](x, contractive_x[i])\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        return self.final_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Nx1cjV-S1w-A"
   },
   "outputs": [],
   "source": [
    "gen = Generator(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(torch.randn(1, 3, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, hidden_channels=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=1)\n",
    "        self.contract1 = DownSampleConv(hidden_channels, use_bn=False)\n",
    "        self.contract2 = DownSampleConv(hidden_channels * 2)\n",
    "        self.contract3 = DownSampleConv(hidden_channels * 4)\n",
    "        self.contract4 = DownSampleConv(hidden_channels * 8)\n",
    "        self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "        x0 = self.conv1(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.contract3(x2)\n",
    "        x4 = self.contract4(x3)\n",
    "        xn = self.final(x4)\n",
    "        return xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = PatchGAN(3*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16, 16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc(torch.randn(1, 3, 256, 256), torch.randn(1, 3, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bWnq9f9z1w-D"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G82gCfZk4u-u"
   },
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(3, 256, 256)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jP5OZVZU1w-D"
   },
   "outputs": [],
   "source": [
    "class Pix2Pix(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 hidden_channels=32,\n",
    "                 depth=6,\n",
    "                 learning_rate=0.0002,\n",
    "                 lambda_recon=200):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gen = Generator(in_channels, out_channels, hidden_channels, depth)\n",
    "        self.patch_gan = PatchGAN(in_channels + out_channels, hidden_channels=8)\n",
    "\n",
    "        # intializing weights\n",
    "        self.gen = self.gen.apply(_weights_init)\n",
    "        self.patch_gan = self.patch_gan.apply(_weights_init)\n",
    "\n",
    "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "\n",
    "    def _gen_step(self, real_images, conditioned_images):\n",
    "        # Pix2Pix has adversarial and a reconstruction loss\n",
    "        # First calculate the adversarial loss\n",
    "        fake_images = self.gen(conditioned_images)\n",
    "        disc_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        lambda_recon = self.hparams.lambda_recon\n",
    "\n",
    "        return adversarial_loss + lambda_recon * recon_loss\n",
    "\n",
    "    def _disc_step(self, real_images, conditioned_images):\n",
    "        fake_images = self.gen(conditioned_images).detach()\n",
    "        fake_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "\n",
    "        real_logits = self.patch_gan(real_images, conditioned_images)\n",
    "\n",
    "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
    "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.learning_rate\n",
    "        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n",
    "        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr)\n",
    "        return disc_opt, gen_opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch\n",
    "\n",
    "        loss = None\n",
    "        if optimizer_idx == 0:\n",
    "            loss = self._disc_step(real, condition)\n",
    "            self.log('PatchGAN Loss', loss)\n",
    "        elif optimizer_idx == 1:\n",
    "            loss = self._gen_step(real, condition)\n",
    "            self.log('Generator Loss', loss)\n",
    "        \n",
    "        if self.current_epoch%50==0 and batch_idx==0 and optimizer_idx==1:\n",
    "            fake = self.gen(condition).detach()\n",
    "            show_tensor_images(condition[0])\n",
    "            show_tensor_images(real[0])\n",
    "            show_tensor_images(fake[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AlZOuMvP1w-E"
   },
   "outputs": [],
   "source": [
    "pix2pix = Pix2Pix(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6z_hPpv1w-E",
    "outputId": "906e8210-5a37-497e-ec36-fec409783ae0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1000, gpus=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MBYbnUsvXNs1"
   },
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(\"epoch-347.ckpt\")\n",
    "# pix2pix = Pix2Pix.load_from_checkpoint(\"epoch-347.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995,
     "referenced_widgets": [
      "7d2a52efbe8e42e28079f529561f6c0d",
      "d985d12c31a449869d3271fc1f375575",
      "4d655f6220ec463e89545d15f247c7e2",
      "5d6623b410dc4996b826f9ecad8d8e0c",
      "5642db348ba94803b9b08dae20a12c67",
      "03b4245f43604f16b08dcc9fae4911d8",
      "819bd7b3c3514486b489089d67b1a312",
      "28dab3f9d3bc4b77b0f00fbd024b9a8b"
     ]
    },
    "id": "kJsjXk1C1w-E",
    "outputId": "72bcc223-817c-4b65-a5e4-afef9c3300f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                  | Type              | Params\n",
      "------------------------------------------------------------\n",
      "0 | gen                   | Generator         | 117 M \n",
      "1 | patch_gan             | PatchGAN          | 294 K \n",
      "2 | adversarial_criterion | BCEWithLogitsLoss | 0     \n",
      "3 | recon_criterion       | L1Loss            | 0     \n",
      "------------------------------------------------------------\n",
      "117 M     Trainable params\n",
      "294 K     Non-trainable params\n",
      "117 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf9bce8065b434385abbd2d5bf4b291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(pix2pix, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpx5O2sXWgGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrzeNY90XAHG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pix2Pix_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03b4245f43604f16b08dcc9fae4911d8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28dab3f9d3bc4b77b0f00fbd024b9a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d655f6220ec463e89545d15f247c7e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Epoch 0:  50%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03b4245f43604f16b08dcc9fae4911d8",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5642db348ba94803b9b08dae20a12c67",
      "value": 50
     }
    },
    "5642db348ba94803b9b08dae20a12c67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d6623b410dc4996b826f9ecad8d8e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28dab3f9d3bc4b77b0f00fbd024b9a8b",
      "placeholder": "​",
      "style": "IPY_MODEL_819bd7b3c3514486b489089d67b1a312",
      "value": " 50/100 [00:13&lt;00:13,  3.67it/s, loss=9.39, v_num=9]"
     }
    },
    "7d2a52efbe8e42e28079f529561f6c0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d655f6220ec463e89545d15f247c7e2",
       "IPY_MODEL_5d6623b410dc4996b826f9ecad8d8e0c"
      ],
      "layout": "IPY_MODEL_d985d12c31a449869d3271fc1f375575"
     }
    },
    "819bd7b3c3514486b489089d67b1a312": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d985d12c31a449869d3271fc1f375575": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
